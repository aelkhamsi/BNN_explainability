{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MNIST data\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.1337,], [0.3086,])]) #?\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\"\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\"\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1     0     4     5\n"
     ]
    }
   ],
   "source": [
    "# Data visualization\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j].item() for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct the neural network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3826ac0fd2d14352bf9c3f4a9621c52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.096\n",
      "[1,  4000] loss: 0.529\n",
      "[1,  6000] loss: 0.425\n",
      "[1,  8000] loss: 0.375\n",
      "[1, 10000] loss: 0.336\n",
      "[1, 12000] loss: 0.339\n",
      "[1, 14000] loss: 0.306\n",
      "\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "\n",
    "LOAD_MODEL=True #Load a pre-trained model if it exists. Set to False to re-train the model\n",
    "\n",
    "def train():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    for epoch in range(1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in tqdm(enumerate(trainloader)):\n",
    "            images, labels = data\n",
    "            #Forward Pass\n",
    "            outputs = net(images)\n",
    "            #Calculating loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            #Backward pass & update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    \n",
    "PATH = 'cnn_mnist.pth'\n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(PATH))\n",
    "    except Exception:\n",
    "        train()\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "else:\n",
    "    train()\n",
    "    torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeb21d1285049128b0ccf9c1971716e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 92 %\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "\n",
    "def test():   \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
